import typing as ty

from fastapi import APIRouter, Depends
from fastapi.responses import RedirectResponse, StreamingResponse
from starlette import status

from askgpt.api.dependencies import ParsedToken, throttle_user_request
from askgpt.api.model import OK, EntityDeleted, RequestBody, Response, ResponseData
from askgpt.feat.factory import GPTService, gpt_service_factory
from askgpt.feat.gpt.model import ChatSession
from askgpt.feat.gpt.params import ChatGPTRoles, CompletionModels

gpt_router = APIRouter(prefix="/gpt")
openai_router = APIRouter(prefix="/openai")

Service = ty.Annotated[GPTService, Depends(gpt_service_factory)]


class ChatCompletionRequest(RequestBody):
    """
        Creates a model response for the given chat conversation.

    Args:
      messages: A list of messages comprising the conversation so far.
          [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).

      model: ID of the model to use. See the
          [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)
          table for details on which models work with the Chat API.

      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
          existing frequency in the text so far, decreasing the model's likelihood to
          repeat the same line verbatim.

          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)


      logit_bias: Modify the likelihood of specified tokens appearing in the completion.

          Accepts a JSON object that maps tokens (specified by their token ID in the
          tokenizer) to an associated bias value from -100 to 100. Mathematically, the
          bias is added to the logits generated by the model prior to sampling. The exact
          effect will vary per model, but values between -1 and 1 should decrease or
          increase likelihood of selection; values like -100 or 100 should result in a ban
          or exclusive selection of the relevant token.

      max_tokens: The maximum number of [tokens](/tokenizer) to generate in the chat completion.

          The total length of input tokens and generated tokens is limited by the model's
          context length.
          [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
          for counting tokens.

      n: How many chat completion choices to generate for each input message.

      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
          whether they appear in the text so far, increasing the model's likelihood to
          talk about new topics.

          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)

      response_format: An object specifying the format that the model must output. Used to enable JSON
          mode.

      seed: This feature is in Beta. If specified, our system will make a best effort to
          sample deterministically, such that repeated requests with the same `seed` and
          parameters should return the same result. Determinism is not guaranteed, and you
          should refer to the `system_fingerprint` response parameter to monitor changes
          in the backend.

      stop: Up to 4 sequences where the API will stop generating further tokens.

      stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be
          sent as data-only
          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
          as they become available, with the stream terminated by a `data: [DONE]`
          message.
          [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).

      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
          make the output more random, while lower values like 0.2 will make it more
          focused and deterministic.

          We generally recommend altering this or `top_p` but not both.

      tool_choice: Controls which (if any) function is called by the model. `none` means the model
          will not call a function and instead generates a message. `auto` means the model
          can pick between generating a message or calling a function. Specifying a
          particular function via
          `{"type: "function", "function": {"name": "my_function"}}` forces the model to
          call that function.

          `none` is the default when no functions are present. `auto` is the default if
          functions are present.

      tools: A list of tools the model may call. Currently, only functions are supported as a
          tool. Use this to provide a list of functions the model may generate JSON inputs
          for.

      top_p: An alternative to sampling with temperature, called nucleus sampling, where the
          model considers the results of the tokens with top_p probability mass. So 0.1
          means only the tokens comprising the top 10% probability mass are considered.

          We generally recommend altering this or `temperature` but not both.

      user: A unique identifier representing your end-user, which can help OpenAI to monitor
          and detect abuse.
          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

      extra_headers: Send extra headers

      extra_query: Add additional query parameters to the request

      extra_body: Add additional JSON properties to the request

      timeout: Override the client-level default timeout for this request, in seconds
    """

    question: str
    role: ChatGPTRoles = "user"
    model: CompletionModels = "gpt-3.5-turbo"
    frequency_penalty: float | None = None
    logit_bias: dict[str, int] | None = None
    max_tokens: int | None = None
    n: int | None = None
    presence_penalty: float | None = None
    response_format: ty.Any = None
    seed: int | None = None
    stop: str | None | list[str] = None
    stream: bool | None = None
    temperature: float | None = None
    tool_choice: ty.Any = None
    tools: list[ty.Any] | None = None
    top_p: float | None = None
    user: str | None = None
    extra_headers: ty.Mapping[str, str | ty.Literal[False]] | None = None
    extra_query: ty.Mapping[str, object] | None = None
    extra_body: dict[str, ty.Any] | None = None
    timeout: float | None = 120  # TODO: preconfig ai client

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "model": "gpt-4-1106-preview",
                    "role": "user",
                    "question": "enter your question here",
                    "stream": True,
                },
            ]
        }
    }


class SessionRenameRequest(RequestBody):
    name: str


class PublicSessionInfo(ResponseData):
    session_id: str
    session_name: str


class PublicChatMessage(ResponseData):
    role: ChatGPTRoles
    content: str


class PublicChatSession(ResponseData):
    user_id: str
    session_id: str
    session_name: str
    messages: list[PublicChatMessage]

    @classmethod
    def from_chat(cls, chat: ChatSession) -> ty.Self:
        return cls.model_construct(
            user_id=chat.user_id,
            session_id=chat.entity_id,
            session_name=chat.session_name,
            messages=[
                PublicChatMessage.model_construct(role=m.role, content=m.content)
                for m in chat.messages
            ],
        )


@openai_router.post("/sessions", response_model=PublicChatSession)
async def create_session(service: Service, token: ParsedToken):
    # TODO: limit rate
    chat_session = await service.create_session(user_id=token.sub)

    return RedirectResponse(
        f"/v1/gpt/openai/sessions/{chat_session.entity_id}",
        status_code=status.HTTP_201_CREATED,
    )


@openai_router.get("/sessions", response_model=list[PublicSessionInfo])
async def list_sessions(service: Service, token: ParsedToken):
    user_sessions = await service.list_sessions(user_id=token.sub)
    public_sessions = [
        PublicSessionInfo(session_id=ss.entity_id, session_name=ss.session_name)
        for ss in user_sessions
    ]
    return public_sessions


@openai_router.get("/sessions/{session_id}")
async def get_session(
    service: Service, session_id: str, token: ParsedToken
) -> PublicChatSession:
    session_actor = await service.get_session_actor(
        user_id=token.sub, session_id=session_id
    )
    chat = PublicChatSession.from_chat(session_actor.entity)
    return chat


@openai_router.put("/sessions/{session_id}")
async def rename_session(
    service: Service, session_id: str, req: SessionRenameRequest, token: ParsedToken
) -> Response:
    await service.rename_session(session_id=session_id, new_name=req.name)
    return OK


@openai_router.delete("/sessions/{session_id}", status_code=204)
async def delete_session(service: Service, session_id: str, token: ParsedToken):
    await service.delete_session(session_id=session_id)
    return EntityDeleted


@openai_router.post(
    "/chat/{session_id}",
    dependencies=[Depends(throttle_user_request)],
    response_class=StreamingResponse,
)
async def chat(
    service: Service, session_id: str, req: ChatCompletionRequest, token: ParsedToken
) -> StreamingResponse:
    data = req.model_dump(exclude_none=True)
    data.setdefault("user", token.sub)

    stream_ans = await service.chatcomplete(
        user_id=token.sub,
        session_id=session_id,
        gpt_type="openai",
        role=data.pop("role"),
        question=data.pop("question"),
        options=data,
    )
    return StreamingResponse(stream_ans, media_type="text/event-stream")


gpt_router.include_router(openai_router)
