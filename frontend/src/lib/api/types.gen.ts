// This file is auto-generated by @hey-api/openapi-ts

export type Body_auth_login = {
    grant_type?: (string | null);
    username: string;
    password: string;
    scope?: string;
    client_id?: (string | null);
    client_secret?: (string | null);
};

/**
 *     Creates a model response for the given chat conversation.
 *
 * Args:
 * messages: A list of messages comprising the conversation so far.
 * [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
 *
 * model: ID of the model to use. See the
 * [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)
 * table for details on which models work with the Chat API.
 *
 * frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
 * existing frequency in the text so far, decreasing the model's likelihood to
 * repeat the same line verbatim.
 *
 * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)
 *
 *
 * logit_bias: Modify the likelihood of specified tokens appearing in the completion.
 *
 * Accepts a JSON object that maps tokens (specified by their token ID in the
 * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
 * bias is added to the logits generated by the model prior to sampling. The exact
 * effect will vary per model, but values between -1 and 1 should decrease or
 * increase likelihood of selection; values like -100 or 100 should result in a ban
 * or exclusive selection of the relevant token.
 *
 * max_tokens: The maximum number of [tokens](/tokenizer) to generate in the chat completion.
 *
 * The total length of input tokens and generated tokens is limited by the model's
 * context length.
 * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
 * for counting tokens.
 *
 * n: How many chat completion choices to generate for each input message.
 *
 * presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
 * whether they appear in the text so far, increasing the model's likelihood to
 * talk about new topics.
 *
 * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)
 *
 * response_format: An object specifying the format that the model must output. Used to enable JSON
 * mode.
 *
 * seed: This feature is in Beta. If specified, our system will make a best effort to
 * sample deterministically, such that repeated requests with the same `seed` and
 * parameters should return the same result. Determinism is not guaranteed, and you
 * should refer to the `system_fingerprint` response parameter to monitor changes
 * in the backend.
 *
 * stop: Up to 4 sequences where the API will stop generating further tokens.
 *
 * stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be
 * sent as data-only
 * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
 * as they become available, with the stream terminated by a `data: [DONE]`
 * message.
 * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
 *
 * temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
 * make the output more random, while lower values like 0.2 will make it more
 * focused and deterministic.
 *
 * We generally recommend altering this or `top_p` but not both.
 *
 * tool_choice: Controls which (if any) function is called by the model. `none` means the model
 * will not call a function and instead generates a message. `auto` means the model
 * can pick between generating a message or calling a function. Specifying a
 * particular function via
 * `{"type: "function", "function": {"name": "my_function"}}` forces the model to
 * call that function.
 *
 * `none` is the default when no functions are present. `auto` is the default if
 * functions are present.
 *
 * tools: A list of tools the model may call. Currently, only functions are supported as a
 * tool. Use this to provide a list of functions the model may generate JSON inputs
 * for.
 *
 * top_p: An alternative to sampling with temperature, called nucleus sampling, where the
 * model considers the results of the tokens with top_p probability mass. So 0.1
 * means only the tokens comprising the top 10% probability mass are considered.
 *
 * We generally recommend altering this or `temperature` but not both.
 *
 * user: A unique identifier representing your end-user, which can help OpenAI to monitor
 * and detect abuse.
 * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).
 *
 * extra_headers: Send extra headers
 *
 * extra_query: Add additional query parameters to the request
 *
 * extra_body: Add additional JSON properties to the request
 *
 * timeout: Override the client-level default timeout for this request, in seconds
 */
export type ChatCompletionRequest = {
    question: string;
    role: 'system' | 'user' | 'assistant' | 'function';
    model?: 'gpt-3.5-turbo' | 'gpt-3.5-turbo-16k' | 'gpt-4' | 'gpt-4-32k' | 'gpt-4-1106-preview' | 'gpt-4-vision-preview';
    frequency_penalty?: (number | null);
    logit_bias?: ({
    [key: string]: (number);
} | null);
    max_tokens?: (number | null);
    n?: (number | null);
    presence_penalty?: (number | null);
    response_format?: unknown;
    seed?: (number | null);
    stop?: (string | Array<(string)> | null);
    stream?: (boolean | null);
    temperature?: (number | null);
    tool_choice?: unknown;
    tools?: (Array<unknown> | null);
    top_p?: (number | null);
    user?: (string | null);
    extra_headers?: ({
    [key: string]: (string | false);
} | null);
    extra_query?: ({
    [key: string]: unknown;
} | null);
    extra_body?: ({
    [key: string]: unknown;
} | null);
    timeout?: (number | null);
};

export type role = 'system' | 'user' | 'assistant' | 'function';

export type model = 'gpt-3.5-turbo' | 'gpt-3.5-turbo-16k' | 'gpt-4' | 'gpt-4-32k' | 'gpt-4-1106-preview' | 'gpt-4-vision-preview';

export type CreateNewKey = {
    api_key: string;
    api_type?: SupportedGPTs;
};

export type HTTPValidationError = {
    detail?: Array<ValidationError>;
};

export type PublicChatMessage = {
    role: 'system' | 'user' | 'assistant' | 'function';
    content: string;
};

export type PublicChatSession = {
    user_id: string;
    session_id: string;
    session_name: string;
    messages: Array<PublicChatMessage>;
};

export type PublicSessionInfo = {
    session_id: string;
    session_name: string;
};

export type PublicUserInfo = {
    user_id: string;
    user_name: string;
    email: string;
};

export type SessionRenameRequest = {
    name: string;
};

export type SignUp = {
    user_name?: string;
    email: string;
    password: string;
};

export type SupportedGPTs = 'openai' | 'askgpt_test';

export type TokenResponse = {
    access_token: string;
    token_type?: 'bearer';
};

export type token_type = 'bearer';

export type UserAuth = {
    user_id: string;
    role?: UserRoles;
    credential: UserCredential;
    last_login: string;
    is_active?: boolean;
};

export type UserCredential = {
    user_name?: string;
    user_email: string;
    hash_password: string;
};

export type UserRoles = 'admin' | 'user';

export type ValidationError = {
    loc: Array<(string | number)>;
    msg: string;
    type: string;
};

export type HealthCheckResponse = (unknown);

export type HealthCheckError = unknown;

export type LoginData = {
    body: Body_auth_login;
};

export type LoginResponse = (TokenResponse);

export type LoginError = (HTTPValidationError);

export type SignupData = {
    body: SignUp;
};

export type SignupResponse = (unknown);

export type SignupError = (HTTPValidationError);

export type FindUserByEmailData = {
    query: {
        email: string;
    };
};

export type FindUserByEmailResponse = (PublicUserInfo);

export type FindUserByEmailError = (HTTPValidationError);

export type GetPublicUserResponse = (PublicUserInfo);

export type GetPublicUserError = unknown;

export type GetUserDetailData = {
    path: {
        user_id: string;
    };
};

export type GetUserDetailResponse = ((UserAuth | null));

export type GetUserDetailError = (HTTPValidationError);

export type DeleteUserResponse = (unknown);

export type DeleteUserError = unknown;

export type CreateNewKeyData = {
    body: CreateNewKey;
};

export type CreateNewKeyResponse = (unknown);

export type CreateNewKeyError = (HTTPValidationError);

export type ListSessionsResponse = (Array<PublicSessionInfo>);

export type ListSessionsError = unknown;

export type CreateSessionResponse = (PublicChatSession);

export type CreateSessionError = unknown;

export type GetSessionData = {
    path: {
        session_id: string;
    };
};

export type GetSessionResponse = (PublicChatSession);

export type GetSessionError = (HTTPValidationError);

export type RenameSessionData = {
    body: SessionRenameRequest;
    path: {
        session_id: string;
    };
};

export type RenameSessionResponse = (unknown);

export type RenameSessionError = (HTTPValidationError);

export type DeleteSessionData = {
    path: {
        session_id: string;
    };
};

export type DeleteSessionResponse = (void);

export type DeleteSessionError = (HTTPValidationError);

export type ChatData = {
    body: ChatCompletionRequest;
    path: {
        session_id: string;
    };
};

export type ChatResponse = (unknown);

export type ChatError = (HTTPValidationError);

export type GenerateErrorPageData = {
    query?: {
        error_type?: string;
    };
};

export type GenerateErrorPageResponse = (string);

export type GenerateErrorPageError = (HTTPValidationError);